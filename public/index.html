<!doctype html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: April 8, 2025 --><html lang="en-us" dir="ltr"
      data-wc-theme-default="system">
  
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="generator" content="Hugo Blox Builder 0.3.1" />

  
  












  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Jeon Ho Kang" />

  
  
  
    
  
  <meta name="description" content="The highly-customizable Hugo Academic theme powered by Hugo Blox Builder. Easily create your personal academic website." />

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/" />

  
  
  
  
    
    <link rel="stylesheet" href="/css/themes/blue.min.css" />
  

  
  
    
    <link href="/dist/wc.min.css" rel="stylesheet" />
  

  
  
  

  
    
    <link href="/css/custom.min.945ba83475e9bb7db055aa7294bd0aa8e3698017b5e129b41449742a40902d76.css" rel="stylesheet" />
  

  <script>
     
    window.hbb = {
       defaultTheme: document.documentElement.dataset.wcThemeDefault,
       setDarkTheme: () => {
        document.documentElement.classList.add("dark");
        document.documentElement.style.colorScheme = "dark";
      },
       setLightTheme: () => {
        document.documentElement.classList.remove("dark");
        document.documentElement.style.colorScheme = "light";
      }
    }

    console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`);

    if ("wc-color-theme" in localStorage) {
      localStorage.getItem("wc-color-theme") === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
    } else {
      window.hbb.defaultTheme === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      if (window.hbb.defaultTheme === "system") {
        window.matchMedia("(prefers-color-scheme: dark)").matches ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      }
    }
  </script>

  <script>
    
    document.addEventListener('DOMContentLoaded', function () {
      
      let checkboxes = document.querySelectorAll('li input[type=\'checkbox\'][disabled]');
      checkboxes.forEach(e => {
        e.parentElement.parentElement.classList.add('task-list');
      });

      
      const liNodes = document.querySelectorAll('.task-list li');
      liNodes.forEach(nodes => {
        let textNodes = Array.from(nodes.childNodes).filter(node => node.nodeType === 3 && node.textContent.trim().length > 1);
        if (textNodes.length > 0) {
          const span = document.createElement('label');
          textNodes[0].after(span);  
          span.appendChild(nodes.querySelector('input[type=\'checkbox\']'));
          span.appendChild(textNodes[0]);
        }
      });
    });
  </script>

  
  
  




































  
  
    <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Jeon Ho Kang" />
  

  
  <link rel="icon" type="image/png" href="/media/icon_hu_13839f9454fa1f8d.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu_aed61f381034ff3d.png" />

  <link rel="canonical" href="http://localhost:1313/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
    <meta property="twitter:site" content="@GetResearchDev" />
    <meta property="twitter:creator" content="@GetResearchDev" />
  
  <meta property="og:site_name" content="Jeon Ho Kang" />
  <meta property="og:url" content="http://localhost:1313/" />
  <meta property="og:title" content="Jeon Ho Kang" />
  <meta property="og:description" content="The highly-customizable Hugo Academic theme powered by Hugo Blox Builder. Easily create your personal academic website." /><meta property="og:image" content="http://localhost:1313/media/icon_hu_a837b1033d1a05be.png" />
    <meta property="twitter:image" content="http://localhost:1313/media/icon_hu_a837b1033d1a05be.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2022-10-24T00:00:00&#43;00:00" />
    
  

  

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite","url": "http://localhost:1313/"
}
</script>


  


  <title>Jeon Ho Kang</title>

  
  
  
  
  
    
    
  
  
  <style>
    @font-face {
      font-family: 'Inter var';
      font-style: normal;
      font-weight: 100 900;
      font-display: swap;
      src: url(/dist/font/Inter.var.woff2) format(woff2);
    }
  </style>

  

  
  


  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  



















  
  
  
  
  
  
  
  <script
    defer
    src="/js/hugo-blox-en.min.js"
    integrity=""
  ></script>

  
  








  
    
      
      <script async defer src="https://buttons.github.io/buttons.js"></script>

      
    
  




</head>

  <body class="dark:bg-hb-dark dark:text-white page-wrapper" id="top">
    <div id="page-bg"></div>
    <div class="page-header sticky top-0 z-30">
      
      
      
        
        
        
          <header id="site-header" class="header">
  <nav class="navbar px-3 flex justify-left">
    <div class="order-0 h-100">
      
      <a class="navbar-brand" href="/" title="Jeon Ho Kang">
        Jeon Ho Kang
      </a>
    </div>
    
    <input id="nav-toggle" type="checkbox" class="hidden" />
    <label
      for="nav-toggle"
      class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1">
      <svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20">
        <title>Open Menu</title>
        <path d="M0 3h20v2H0V3z m0 6h20v2H0V9z m0 6h20v2H0V0z"></path>
      </svg>
      <svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20">
        <title>Close Menu</title>
        <polygon
          points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2"
          transform="rotate(45 10 10)"></polygon>
      </svg>
    </label>
    

    
    
    <ul
      id="nav-menu"
      class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left
      ">
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#home"
        >Home</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#research"
        >Research</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#publications"
        >Publications</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#projects"
        >Projects</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#experience"
        >Experience</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#awards"
        >Accomplishments</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/uploads/cv2_web_jeonhokang.pdf"
        >CV</a
        >
      </li>
      
      
      
    </ul>

    <div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0">

      
      
      

      
      

      
      

      
      
    </div>
  </nav>
</header>



        
      
    </div>
    <div class="page-body ">
      

  
  
    








  

  

  

  

  


















  



  
  











  
    
    
    
  





























<section id="home" class="relative hbb-section blox-resume-biography-3  dark main" style="padding: 6rem 0 6rem 0;" >
 <div class="home-section-bg " style="background-color: black;background-image: linear-gradient(30deg, #282B2C, #2c3e50);">
   
 </div>
  

  

    

















<div class="resume-biography px-3 flex flex-col md:flex-row justify-center gap-12">
  <div class="flex-none m-w-[130px] mx-auto md:mx-0">

  
  

  <div id="profile" class="flex justify-center items-center flex-col">
    

    <div class="avatar-wrapper mt-10">
      
      <img class="avatar rounded-full bg-white dark:bg-gray-800 p-1" src="/author/jeon-ho-kang/avatar_hu_872a9a7c4f9c5ed2.jpg" alt="Jeon Ho Kang"
           width="150" height="150">
      
    </div>
    

    <div class="portrait-title dark:text-white">

      <div class="text-3xl font-bold mb-2 mt-6">Jeon Ho Kang</div>

      <h3 class="font-semibold mb-1">Ph.D. Student in Robotics</h3>

      
      <div class="mb-2">
        <a href="https://sites.usc.edu/rros/" target="_blank" rel="noopener">
        <div>University of Southern California</div>
        </a>
      </div>
      
    </div>

    <ul class="network-icon dark:text-zinc-100">
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a href="https://github.com/JeonHoKang" target="_blank" rel="noopener" aria-label="brands/github"
           >
          <svg style="height: 1.5rem;" fill="currentColor" viewBox="3 3 18 18"><path d="M12 3C7.0275 3 3 7.12937 3 12.2276C3 16.3109 5.57625 19.7597 9.15374 20.9824C9.60374 21.0631 9.77249 20.7863 9.77249 20.5441C9.77249 20.3249 9.76125 19.5982 9.76125 18.8254C7.5 19.2522 6.915 18.2602 6.735 17.7412C6.63375 17.4759 6.19499 16.6569 5.8125 16.4378C5.4975 16.2647 5.0475 15.838 5.80124 15.8264C6.51 15.8149 7.01625 16.4954 7.18499 16.7723C7.99499 18.1679 9.28875 17.7758 9.80625 17.5335C9.885 16.9337 10.1212 16.53 10.38 16.2993C8.3775 16.0687 6.285 15.2728 6.285 11.7432C6.285 10.7397 6.63375 9.9092 7.20749 9.26326C7.1175 9.03257 6.8025 8.08674 7.2975 6.81794C7.2975 6.81794 8.05125 6.57571 9.77249 7.76377C10.4925 7.55615 11.2575 7.45234 12.0225 7.45234C12.7875 7.45234 13.5525 7.55615 14.2725 7.76377C15.9937 6.56418 16.7475 6.81794 16.7475 6.81794C17.2424 8.08674 16.9275 9.03257 16.8375 9.26326C17.4113 9.9092 17.76 10.7281 17.76 11.7432C17.76 15.2843 15.6563 16.0687 13.6537 16.2993C13.98 16.5877 14.2613 17.1414 14.2613 18.0065C14.2613 19.2407 14.25 20.2326 14.25 20.5441C14.25 20.7863 14.4188 21.0746 14.8688 20.9824C16.6554 20.364 18.2079 19.1866 19.3078 17.6162C20.4077 16.0457 20.9995 14.1611 21 12.2276C21 7.12937 16.9725 3 12 3Z"></path></svg>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a href="https://www.linkedin.com/in/jeonhkang/" target="_blank" rel="noopener" aria-label="brands/linkedin"
           >
          <svg style="height: 1.5rem;" xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a href="https://scholar.google.com/citations?user=-EeDGVUAAAAJ&amp;hl=en" target="_blank" rel="noopener" aria-label="academicons/google-scholar"
           >
          <svg style="height: 1.5rem;" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M343.759 106.662V79.43L363.524 64h-213.89L20.476 176.274h85.656a82.339 82.339 0 0 0-.219 6.225c0 20.845 7.22 38.087 21.672 51.861c14.453 13.797 32.252 20.648 53.327 20.648c4.923 0 9.75-.368 14.438-1.024c-2.907 6.5-4.374 12.523-4.374 18.142c0 9.875 4.499 20.43 13.467 31.642c-39.234 2.67-68.061 9.732-86.437 21.163c-10.531 6.5-19 14.704-25.39 24.531c-6.391 9.9-9.578 20.515-9.578 31.962c0 9.648 2.062 18.336 6.219 26.062c4.156 7.726 9.578 14.07 16.312 18.984c6.718 4.968 14.469 9.101 23.219 12.469c8.734 3.344 17.406 5.718 26.061 7.062A167.052 167.052 0 0 0 180.555 448c13.469 0 26.953-1.734 40.547-5.187c13.562-3.485 26.28-8.642 38.171-15.493c11.86-6.805 21.515-16.086 28.922-27.718c7.39-11.68 11.094-24.805 11.094-39.336c0-11.016-2.25-21.039-6.75-30.14c-4.468-9.073-9.938-16.542-16.452-22.345c-6.501-5.813-13-11.155-19.516-15.968c-6.5-4.845-12-9.75-16.468-14.813c-4.485-5.046-6.735-10.054-6.735-14.984c0-4.921 1.734-9.672 5.216-14.265c3.455-4.61 7.674-9.048 12.61-13.306c4.937-4.25 9.875-8.968 14.796-14.133c4.922-5.147 9.141-11.827 12.61-20.008c3.485-8.18 5.203-17.445 5.203-27.757c0-13.453-2.547-24.46-7.547-33.314c-.594-1.022-1.218-1.803-1.875-3.022l56.907-46.672v17.119c-7.393.93-6.624 5.345-6.624 10.635V245.96c0 5.958 4.875 10.834 10.834 10.834h3.989c5.958 0 10.833-4.875 10.833-10.834V117.293c0-5.277.778-9.688-6.561-10.63zm-107.36 222.48c1.14.75 3.704 2.78 7.718 6.038c4.05 3.243 6.797 5.695 8.266 7.414a443.553 443.553 0 0 1 6.376 7.547c2.813 3.375 4.718 6.304 5.718 8.734c1 2.477 2.016 5.461 3.047 8.946a38.27 38.27 0 0 1 1.485 10.562c0 17.048-6.564 29.68-19.656 37.859c-13.125 8.18-28.767 12.274-46.938 12.274c-9.187 0-18.203-1.093-27.063-3.196c-8.843-2.116-17.311-5.336-25.39-9.601c-8.078-4.258-14.577-10.204-19.5-17.797c-4.938-7.64-7.407-16.415-7.407-26.25c0-10.32 2.797-19.29 8.422-26.906c5.594-7.625 12.938-13.391 22.032-17.315c9.063-3.946 18.25-6.742 27.562-8.398a157.865 157.865 0 0 1 28.438-2.555c4.47 0 7.936.25 10.405.696c.455.219 3.032 2.07 7.735 5.563c4.704 3.462 7.625 5.595 8.75 6.384zm-3.359-100.579c-7.406 8.86-17.734 13.288-30.953 13.288c-11.86 0-22.298-4.764-31.266-14.312c-9-9.523-15.422-20.328-19.344-32.43c-3.937-12.109-5.906-23.984-5.906-35.648c0-13.694 3.596-25.352 10.781-34.976c7.187-9.65 17.5-14.485 30.938-14.485c11.875 0 22.374 5.038 31.437 15.157c9.094 10.085 15.61 21.413 19.517 33.968c3.922 12.54 5.873 24.53 5.873 35.984c0 13.446-3.702 24.61-11.076 33.454z"/></svg>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a href="https://www.researchgate.net/profile/Jeon-Kang?ev=hdr_xprf" target="_blank" rel="noopener" aria-label="custom/researchgate"
           >
          <svg style="height: 1.5rem;" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><rect fill="#00d0af" height="512" rx="15%" width="512"/><g fill="#feffff"><path d="m271 383c-15-4-23-10-36-26-9-12-26-39-35-53l-6-11h-24v34c1 43 0 42 19 45l10 1v4 4h-80v-4c0-4 1-4 9-6 10-2 14-5 15-14 1-4 1-31 1-79 0-70-1-72-3-77-3-5-7-7-18-8-4-1-5-1-5-5v-4l43-1c55-1 65 0 81 11 15 10 22 24 20 43-1 21-17 42-37 50-4 1-7 3-7 3 0 2 17 28 28 43 15 21 27 32 36 37 4 2 9 3 10 3 3 0 3 1 3 4s-1 5-2 5c-5 2-19 2-26 0zm-57-109c14-7 22-18 23-35 1-13-2-22-10-30-9-10-25-14-48-12l-10 1v39c0 36 0 40 2 40 1 0 9 1 18 0 14 0 17-1 24-4z"/><path d="m321 228c-25-4-34-20-32-61 1-21 3-30 11-38 7-7 13-9 25-10 13-1 21 2 29 8 5 4 9 10 9 13 0 1-3 2-6 4l-6 3-3-3c-5-6-9-9-14-11-10-3-20 2-25 11-3 5-3 6-3 29 0 22 0 25 3 29 4 7 12 11 21 10 13-1 20-10 20-24v-7h-10-10v-13h36v15c0 12-1 16-3 22-6 15-23 24-42 22z"/></g></svg>
        </a>
      </li>
      
    </ul>
  </div>
  </div>

  <div class="flex-auto max-w-prose md:mt-12">
    
    <div class="pt-2 justify-content-center prose prose-slate dark:prose-invert">
      <div class="bio-text" >
        <p>I am a Ph.D. student in Robotics at the University of California, fortunate to be advised by <a href="https://sites.usc.edu/skgupta/" target="_blank" rel="noopener">Professor S.K. Gupta</a>. I am a member of the <a href="https://sites.usc.edu/rros/" target="_blank" rel="noopener">RROS Lab</a>, where we focus on smart manufacturing systems and skill learning for complex manufacturing tasks.</p>
<p>My research centers on leveraging probabilistic models and deep learning to enable multi-sensory perception and manipulation skills, with the goal of enhancing flexibility and adaptability in robot learning. I am particularly interested in developing intelligent ways to integrate diverse sensory feedback — including force, tactile inputs, and language — into robotic systems.</p>
<p>I have authored three first-author publications, including papers in ICRA and RA-L, with one currently under review for CASE 2025. Additionally, I have contributed to three ASME conference papers (MSEC and IDETC), with our MSEC 2024 paper receiving the Best Conference Paper Award (2nd place). I have also served as a reviewer for multiple conferences, including ICRA 2025, IROS 2025, and CASE 2025.</p>

      </div>
    </div>
    

    
    <a href="/uploads/CV2_Web_JeonHoKang.pdf" target="_blank" class="inline-flex items-center px-4 py-2 text-sm font-medium text-gray-900 bg-white border border-gray-200 rounded-lg hover:bg-gray-100 hover:text-primary-700 focus:z-10 focus:ring-4 focus:outline-none focus:ring-gray-200 focus:text-primary-700 dark:bg-gray-800 dark:text-gray-300 dark:border-gray-600 dark:hover:text-white dark:hover:bg-gray-700 dark:focus:ring-gray-700"><svg class="w-3.5 h-3.5 me-2.5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="currentColor" viewBox="0 0 20 20">
      <path d="M14.707 7.793a1 1 0 0 0-1.414 0L11 10.086V1.5a1 1 0 0 0-2 0v8.586L6.707 7.793a1 1 0 1 0-1.414 1.414l4 4a1 1 0 0 0 1.416 0l4-4a1 1 0 0 0-.002-1.414Z"/>
      <path d="M18 12h-2.55l-2.975 2.975a3.5 3.5 0 0 1-4.95 0L4.55 12H2a2 2 0 0 0-2 2v4a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2v-4a2 2 0 0 0-2-2Zm-3 5a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"/>
    </svg> Download CV</a>
    




  <div class="grid grid-cols-1 md:grid-cols-2 gap-4 justify-between mt-6 dark:text-gray-300">

    
    <div class="">
      <div class="section-subheading mb-3">Interests</div>
      <ul class="list-disc list-inside space-y-1 pl-5">
        
        <li>
          Deep Learning
        </li>
        
        <li>
          Robotics
        </li>
        
        <li>
          Sensory Fusion
        </li>
        
        <li>
          Imitation Learning
        </li>
        
        <li>
          Motion Planning
        </li>
        
        <li>
          Task Planning
        </li>
        
      </ul>
    </div>
    

    
    <div class="">
      <div class="section-subheading mb-3">Education</div>
      <ul class="">
        
        <li class="flex items-start gap-3">
          <svg style="" class='flex-shrink-0 w-5 h-5 me-2 mt-1' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M4.26 10.147a60.436 60.436 0 0 0-.491 6.347A48.627 48.627 0 0 1 12 20.904a48.627 48.627 0 0 1 8.232-4.41a60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.57 50.57 0 0 0-2.658-.813A59.905 59.905 0 0 1 12 3.493a59.902 59.902 0 0 1 10.399 5.84a51.39 51.39 0 0 0-2.658.814m-15.482 0A50.697 50.697 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5a.75.75 0 0 0 0 1.5m0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"/></svg>
          <div class="description">
            <p class="course">Ph.D. in AME (Robotics) 2026</p>
            <p class="text-sm">University of Southern California</p>
          </div>
        </li>
        
        <li class="flex items-start gap-3">
          <svg style="" class='flex-shrink-0 w-5 h-5 me-2 mt-1' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M4.26 10.147a60.436 60.436 0 0 0-.491 6.347A48.627 48.627 0 0 1 12 20.904a48.627 48.627 0 0 1 8.232-4.41a60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.57 50.57 0 0 0-2.658-.813A59.905 59.905 0 0 1 12 3.493a59.902 59.902 0 0 1 10.399 5.84a51.39 51.39 0 0 0-2.658.814m-15.482 0A50.697 50.697 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5a.75.75 0 0 0 0 1.5m0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"/></svg>
          <div class="description">
            <p class="course">MS in Mechanical Engineering (Automation) 2023</p>
            <p class="text-sm">University of Southern California</p>
          </div>
        </li>
        
        <li class="flex items-start gap-3">
          <svg style="" class='flex-shrink-0 w-5 h-5 me-2 mt-1' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M4.26 10.147a60.436 60.436 0 0 0-.491 6.347A48.627 48.627 0 0 1 12 20.904a48.627 48.627 0 0 1 8.232-4.41a60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.57 50.57 0 0 0-2.658-.813A59.905 59.905 0 0 1 12 3.493a59.902 59.902 0 0 1 10.399 5.84a51.39 51.39 0 0 0-2.658.814m-15.482 0A50.697 50.697 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5a.75.75 0 0 0 0 1.5m0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"/></svg>
          <div class="description">
            <p class="course">BS Mechanical Engineering 2022</p>
            <p class="text-sm">University of Southern California</p>
          </div>
        </li>
        
      </ul>
    </div>
    

  </div>

</div>
</div>


  

  
</section>

  

  
  
    








  

  

  

  

  


















  



  
  











  
    
    
    
  






  
























<section id="news" class="relative hbb-section blox-markdown  custom-bullets" style="padding: 6rem 0 6rem 0;font-size: 2.0rem; text-align: left; width: 100%; max-width: none" >
 <div class="home-section-bg " style="background-color: light;background-image: linear-gradient(60deg, #282B2C, #2c3e50);">
   
 </div>
  

  

    









<div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6">

  <div class="mb-6 text-3xl font-bold text-gray-900 dark:text-white">
    
  </div>

  
  <div class="prose prose-slate lg:prose-xl dark:prose-invert max-w-prose"><h2 id="news">News</h2>
<ul>
<li><strong>April 2025</strong>: Now our paper, code and dataset can be accessed via <a href="https://arxiv.org/abs/2503.03998" target="_blank" rel="noopener">Arxiv</a>, <a href="https://cs.paperswithcode.com/paper/robotic-compliant-object-prying-using" target="_blank" rel="noopener">Papers with code</a>, and <a href="https://huggingface.co/datasets/Jeon-hk/Battery_Prying_Dataset/blob/main/README.md" target="_blank" rel="noopener">Hugging Face</a> !</li>
<li><strong>March 2025</strong>: Looking forward to interning at <strong>Honda Resarch Institute</strong> this summer as Resesarch Scientist Intern working on <strong>Behavior Models for Dexterous Manipulation</strong>!</li>
<li><strong>March 2025</strong>: Our paper on Compliant Object prying has been accepted on Robotics and Automation Letters!</li>
<li><strong>March 2024</strong>: Our paper at MSEC 2024 won the <strong>Best Conference Paper Award</strong>!</li>
<li><strong>February 2024</strong>: Our paper on <strong>Lage Language Moddels for Contingency Recovery and Task Allocation</strong> has been accepted for publication at <strong>ICRA 2024</strong>!</li>
</ul>
</div>
</div>


  

  
</section>

  

  
  
    








  

  

  

  

  




















  
  











  
    
    
    
  





























<section id="research" class="relative hbb-section blox-collection  dark" style="padding: 6rem 0 6rem 0;" >
 <div class="home-section-bg " style="background-image: linear-gradient(30deg, #282B2C, #2c3e50);">
   
 </div>
  

  

    











  









  
  
  
  
  







  
















  









  <div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6 md:px-0">

    <div class="mb-6 text-3xl font-bold text-gray-900 dark:text-white">
      Research
    </div>

    
  </div>
  

  <div class="flex flex-col items-center px-6">

    
    


<div class="container px-8 mx-auto xl:px-5 py-5 lg:py-8 max-w-screen-lg ">
  <div class="grid gap-10 md:grid-cols-2 lg:gap-10">


    
    
      <div>
  

    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/publication/case-2025/" >

      <img alt="Task-Context-Aware Diffusion Policy with Language Guidance for Multi-task Disassembly"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/publication/case-2025/featured_hu_d47f45f3f6da0365.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
        <a href="/tags/language-guided-diffusion-policy/"><span
          class="inline-block text-xs font-medium tracking-wider uppercase mt-5 text-primary-700 dark:text-primary-300">Language-Guided Diffusion Policy</span></a>
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/publication/case-2025/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Task-Context-Aware Diffusion Policy with Language Guidance for Multi-task Disassembly
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/publication/case-2025/" >
         </a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2025-04-01">Apr 1, 2025</time>
        </div>
      </div>

    </div>
  </div>
</div>

  </div>
    
    
      <div>
  

    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/publication/ral-2025/" >

      <img alt="Robotic Compliant Object Prying Using Diffusion Policy Guided by Vision and Force Observations"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/publication/ral-2025/featured_hu_ee2abee9f97cbe4a.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
        <a href="/tags/diffusion-models/"><span
          class="inline-block text-xs font-medium tracking-wider uppercase mt-5 text-primary-700 dark:text-primary-300">Diffusion Models</span></a>
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/publication/ral-2025/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Robotic Compliant Object Prying Using Diffusion Policy Guided by Vision and Force Observations
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/publication/ral-2025/" >
         </a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2025-03-01">Mar 1, 2025</time>
        </div>
      </div>

    </div>
  </div>
</div>

  </div>
    
    
      <div>
  

    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/publication/icra-2024/" >

      <img alt="Generating and Applying Contingency Handling Procedures in Human-Robot Teams in Manufacturing Applications"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/publication/icra-2024/featured_hu_7b67c5791fb246ac.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
        <a href="/tags/llm-for-task-planning/"><span
          class="inline-block text-xs font-medium tracking-wider uppercase mt-5 text-primary-700 dark:text-primary-300">LLM for Task Planning</span></a>
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/publication/icra-2024/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Generating and Applying Contingency Handling Procedures in Human-Robot Teams in Manufacturing Applications
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/publication/icra-2024/" >
         </a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2024-05-01">May 1, 2024</time>
        </div>
      </div>

    </div>
  </div>
</div>

  </div>
    
    
      <div>
  

    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/publication/idetc-2023/" >

      <img alt="Safe Robot to Human Tool Handover to Support Effective Collaboration"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/publication/idetc-2023/featured_hu_81e89bb797aceda8.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
        <a href="/tags/tool-manipulation-and-hri/"><span
          class="inline-block text-xs font-medium tracking-wider uppercase mt-5 text-primary-700 dark:text-primary-300">Tool Manipulation and HRI</span></a>
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/publication/idetc-2023/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Safe Robot to Human Tool Handover to Support Effective Collaboration
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/publication/idetc-2023/" >
        </a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2023-08-01">Aug 1, 2023</time>
        </div>
      </div>

    </div>
  </div>
</div>

  </div>
    

      </div>
</div>


  </div>

  
  
  
</body>

<script>
  document.addEventListener('DOMContentLoaded', () => {
    const searchInput = document.getElementById('pubSearch');
    const authorInput = document.getElementById('pubAuthor');
    const yearInput = document.getElementById('pubYear');

    if (searchInput && authorInput && yearInput) {
      const items = document.querySelectorAll('.publication-item');

      function filterItems() {
        const query = searchInput.value.trim().toLowerCase();
        const author = authorInput.value.trim().toLowerCase();
        const year = yearInput.value.trim();

        items.forEach(item => {
          const title = item.dataset.title || "";
          const summary = item.dataset.summary || "";
          const authors = item.dataset.author || "";
          const pubYear = item.dataset.year || "";

          const matchQuery = title.includes(query) || summary.includes(query);
          const matchAuthor = authors.includes(author);
          const matchYear = pubYear.includes(year);

          const shouldShow = matchQuery && matchAuthor && matchYear;
          item.style.display = shouldShow ? "" : "none";
        });
      }

      searchInput.addEventListener("input", filterItems);
      authorInput.addEventListener("input", filterItems);
      yearInput.addEventListener("change", filterItems);
    }
  });
</script>

  

  
</section>

  

  
  
    








  

  

  

  

  


















  



  
  











  
    
    
    
  





























<section id="experience" class="relative hbb-section blox-resume-experience  custom-position dark" style="padding: 6rem 0 6rem 0;" >
 <div class="home-section-bg " style="background-color: black;background-image: linear-gradient(30deg, #282B2C, #2c3e50);">
   
 </div>
  

  

    





















<div class="flex flex-col items-center max-w-prose mx-auto">
  
  <div class="flex flex-col  lg:gap-x-6 w-100 px-6 sm:px-0">

        
        <div class="w-full">
          <h3 class="mb-6 text-4xl font-bold text-gray-900 dark:text-white">Experience</h3>
              

            <ol class="relative border-s border-gray-200 dark:border-gray-700">  
              
              <li class="mb-10 ms-6">
                <span class="absolute flex items-center justify-center w-6 h-6 bg-primary-100 rounded-full -start-3 ring-8 ring-white dark:ring-gray-900 dark:bg-primary-900">
                  <svg fill="currentcolor" viewBox="0 0 256 256" class="w-5 h-5 text-primary-800 dark:text-primary-300">
                    <path d="M216 56H176V48a24 24 0 00-24-24H104A24 24 0 0080 48v8H40A16 16 0 0024 72V2e2a16 16 0 0016 16H216a16 16 0 0016-16V72A16 16 0 00216 56zM96 48a8 8 0 018-8h48a8 8 0 018 8v8H96zM216 72v41.61A184 184 0 01128 136a184.07 184.07.0 01-88-22.38V72zm0 128H40V131.64A200.19 200.19.0 00128 152a200.25 200.25.0 0088-20.37V2e2zM104 112a8 8 0 018-8h32a8 8 0 010 16H112a8 8 0 01-8-8z"></path>
                  </svg>
                </span>  
                  <div class="flex mb-12 items-center">
                      
                      <div class="bg-gray-600 p-6 rounded-2xl shadow-xl w-full text-gray-100 flex items-start gap-4">
                        
                        
                        
                        <div class="flex-shrink-0">
                          <img src="/media/experience/HRIlogo.svg" alt=" logo" class="object-contain rounded-md" style="width: 92px; height: 92px;" loading="lazy">
                        </div>
                        
                  
                        
                        <div class="flex flex-col">
                          <h3 class="text-lg font-semibold">Research Scientist Intern</h3>
                          <p class="text-primary-400 font-medium">Honda Research Institute</p>
                          <p class="text-sm text-gray-400 mt-1">
                            
                              May 2025 –
                            
                            
                              Aug 2025
                            
                            
                          </p>
                          <p class="mt-3 text-sm text-gray-300">Researh on vision language action models applied on dexterous manipulations tasks</p>

                      </div>
                    </div>
                  
                  </div>
              
              </li>
              
              <li class="mb-10 ms-6">
                <span class="absolute flex items-center justify-center w-6 h-6 bg-primary-100 rounded-full -start-3 ring-8 ring-white dark:ring-gray-900 dark:bg-primary-900">
                  <svg fill="currentcolor" viewBox="0 0 256 256" class="w-5 h-5 text-primary-800 dark:text-primary-300">
                    <path d="M216 56H176V48a24 24 0 00-24-24H104A24 24 0 0080 48v8H40A16 16 0 0024 72V2e2a16 16 0 0016 16H216a16 16 0 0016-16V72A16 16 0 00216 56zM96 48a8 8 0 018-8h48a8 8 0 018 8v8H96zM216 72v41.61A184 184 0 01128 136a184.07 184.07.0 01-88-22.38V72zm0 128H40V131.64A200.19 200.19.0 00128 152a200.25 200.25.0 0088-20.37V2e2zM104 112a8 8 0 018-8h32a8 8 0 010 16H112a8 8 0 01-8-8z"></path>
                  </svg>
                </span>  
                  <div class="flex mb-12 items-center">
                      
                      <div class="bg-gray-600 p-6 rounded-2xl shadow-xl w-full text-gray-100 flex items-start gap-4">
                        
                        
                        
                        <div class="flex-shrink-0">
                          <img src="/media/experience/usc.svg" alt=" logo" class="object-contain rounded-md" style="width: 92px; height: 92px;" loading="lazy">
                        </div>
                        
                  
                        
                        <div class="flex flex-col">
                          <h3 class="text-lg font-semibold">Graduate Researcher</h3>
                          <p class="text-primary-400 font-medium">Center for Advanced Manufacturing</p>
                          <p class="text-sm text-gray-400 mt-1">
                            
                              Jan 2022 –
                            
                            
                              Present
                            
                            
                          </p>
                          <p class="mt-3 text-sm text-gray-300">Conduct research on decision making for robot manipulation and task planning for manufacturing and publish to top-tier journals and confereneces.</p>

                      </div>
                    </div>
                  
                  </div>
              
              </li>
              
              <li class="mb-10 ms-6">
                <span class="absolute flex items-center justify-center w-6 h-6 bg-primary-100 rounded-full -start-3 ring-8 ring-white dark:ring-gray-900 dark:bg-primary-900">
                  <svg fill="currentcolor" viewBox="0 0 256 256" class="w-5 h-5 text-primary-800 dark:text-primary-300">
                    <path d="M216 56H176V48a24 24 0 00-24-24H104A24 24 0 0080 48v8H40A16 16 0 0024 72V2e2a16 16 0 0016 16H216a16 16 0 0016-16V72A16 16 0 00216 56zM96 48a8 8 0 018-8h48a8 8 0 018 8v8H96zM216 72v41.61A184 184 0 01128 136a184.07 184.07.0 01-88-22.38V72zm0 128H40V131.64A200.19 200.19.0 00128 152a200.25 200.25.0 0088-20.37V2e2zM104 112a8 8 0 018-8h32a8 8 0 010 16H112a8 8 0 01-8-8z"></path>
                  </svg>
                </span>  
                  <div class="flex mb-12 items-center">
                      
                      <div class="bg-gray-600 p-6 rounded-2xl shadow-xl w-full text-gray-100 flex items-start gap-4">
                        
                        
                        
                  
                        
                        <div class="flex flex-col">
                          <h3 class="text-lg font-semibold">Engineering Intern (Undergraduate)</h3>
                          <p class="text-primary-400 font-medium">Versa Products</p>
                          <p class="text-sm text-gray-400 mt-1">
                            
                              Jan 2022 –
                            
                            
                              May 2022
                            
                            
                          </p>
                          <p class="mt-3 text-sm text-gray-300"></p>

                      </div>
                    </div>
                  
                  </div>
              
              </li>
              
            </ol>
        </div>
        

  </div>


</div>




  

  
</section>

  

  
  
    








  

  

  

  

  


















  



  
  











  
    
    
    
  





























<section id="publications" class="relative hbb-section blox-collection  dark citation-dark" style="padding: 6rem 0 6rem 0;" >
 <div class="home-section-bg " style="background-color: black;background-image: linear-gradient(30deg, #282B2C, #2c3e50);">
   
 </div>
  

  

    











  









  
  
  
  
  






















  







<body class="overflow-x-hidden">
  
      
      
      
        
        
          
        
      
        
        
      
        
        
      
        
        
          
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
          
        
      
      
      
  


  <div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6 md:px-0">

    <div class="mb-6 text-3xl font-bold text-gray-900 dark:text-white">
      Publications
    </div>

    
  </div>
  

  <div class="flex flex-col items-center px-6">

    
    
<div class="mt-16 sm:mt-20 container max-w-3xl w-full">
  <div class="flex flex-col space-y-3">


    
    
      <div class="publication-item"
          data-title="task-context-aware diffusion policy with language guidance for multi-task disassembly"
          data-summary="
diffusion-based policy learning frameworks excel in learning diverse tasks and achieving high success rates. however, in manufacturing settings, success rate alone is insufficient for real-world deployment. tasks must be executed efficiently, minimizing idle time while maintaining precision. additionally, in assembly and disassembly settings, a single scene often contains multiple task goals that need to be completed—such as picking up an engine while simultaneously securing a suspension—requiring the robot to reason over multiple objectives within the same observation space. in human-robot collaboration, enabling humans to specify task preferences is crucial for flexible and intuitive interaction. in this paper, we address two key challenges : (1) improving task execution efficiency by structuring tasks into distinct sub-task modes via language, and (2) enabling human operators to select tasks using natural language commands. additionally, we introduce adaptive parameter selection framework and reliance on different sensory modalities depending on these sub-task modes. we evaluate our approach on the nist task board, a representative benchmark of real-world tasks where multiple task goals exist within the same scene. our method improves execution speed by 57\% and show 19\% improvement in task success rates. demonstration videos are available at [project website](https://rros-lab.github.io/task-aware-diffusion/)
"
          data-author="admin, sagar joshi, neel dhanaraj, satyandra k. gupta"
          data-year="2025">
    

    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="font-bold">
      Jeon Ho Kang</span>, <span >
      Sagar Joshi</span>, <span >
      Neel Dhanaraj</span>, <span >
      Satyandra K. Gupta</span>
  </span>
  (2025).
  <a href="/publication/case-2025/" class="underline">Task-Context-Aware Diffusion Policy with Language Guidance for Multi-task Disassembly</a>.
  2025 IEEE 20th International Conference on Automation Science and Engineering (CASE).
  

  
  <div class="flex flex-wrap space-x-3">
    








  



<a class="hb-attachment-link hb-attachment-small" href="/publication/case-2025/CASE%202025.pdf" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9"/></svg>
  PDF
</a>



<a class="hb-attachment-link hb-attachment-small" href="/publication/case-2025/cite.bib" target="_blank" data-filename="/publication/case-2025/cite.bib">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"/></svg>
  Cite
</a>


  
  
    
  
<a class="hb-attachment-link hb-attachment-small" href="https://github.com/JeonHoKang/CASE2025-Task-Context-Aware-Diffusion-Policy" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M17.25 6.75L22.5 12l-5.25 5.25m-10.5 0L1.5 12l5.25-5.25m7.5-3l-4.5 16.5"/></svg>
  Code
</a>




<a class="hb-attachment-link hb-attachment-small" href="https://rros-lab.github.io/task-aware-diffusion/" target="_blank" rel="noopener">
  Project
</a>




  


  
  
    
  
<a class="hb-attachment-link hb-attachment-small" href="https://www.youtube.com/watch?v=HKWfDn48jw4" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5" d="m15.75 10.5l4.72-4.72a.75.75 0 0 1 1.28.53v11.38a.75.75 0 0 1-1.28.53l-4.72-4.72M4.5 18.75h9a2.25 2.25 0 0 0 2.25-2.25v-9a2.25 2.25 0 0 0-2.25-2.25h-9A2.25 2.25 0 0 0 2.25 7.5v9a2.25 2.25 0 0 0 2.25 2.25Z"/></svg>
  Video
</a>





  </div>
  

  
  
</div>

  </div>
    
    
      <div class="publication-item"
          data-title="robotic compliant object prying using diffusion policy guided by vision and force observations"
          data-summary="the growing adoption of batteries in the electric vehicle industry and various consumer products has created an urgent need for effective recycling solutions. these products often contain a mix of compliant and rigid components, making robotic disassembly a critical step toward achieving scalable recycling processes. diffusion policy has emerged as a promising approach for learning low-level skills in robotics. to effectively apply diffusion policy to contact-rich tasks, incorporating force as feedback is essential. in this paper, we apply diffusion policy with vision and force in a compliant object prying task. however, when combining low-dimensional contact force with high-dimensional image, the force information may be diluted. to address this issue, we propose a method that effectively integrates force with image data for diffusion policy observations. we validate our approach on a battery prying task that demands high precision and multi-step execution. our model achieves a 96% success rate in diverse scenarios, marking a 57% improvement over the vision-only baseline. our method also demonstrates zero-shot transfer capability to handle unseen objects and battery types. supplementary videos and implementation codes are available on our project website -  **[project website](https://rros-lab.github.io/diffusion-with-force.github.io/)**"
          data-author="admin, sagar joshi, ruopeng huang, satyandra k. gupta"
          data-year="2025">
    

    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="font-bold">
      Jeon Ho Kang</span>, <span >
      Sagar Joshi</span>, <span >
      Ruopeng Huang</span>, <span >
      Satyandra K. Gupta</span>
  </span>
  (2025).
  <a href="/publication/ral-2025/" class="underline">Robotic Compliant Object Prying Using Diffusion Policy Guided by Vision and Force Observations</a>.
  Robotics and Automation Letters, 2025.
  

  
  <div class="flex flex-wrap space-x-3">
    








  
    
  



<a class="hb-attachment-link hb-attachment-small" href="https://arxiv.org/abs/2503.03998" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9"/></svg>
  PDF
</a>



<a class="hb-attachment-link hb-attachment-small" href="/publication/ral-2025/cite.bib" target="_blank" data-filename="/publication/ral-2025/cite.bib">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"/></svg>
  Cite
</a>


  
  
    
  
<a class="hb-attachment-link hb-attachment-small" href="https://github.com/JeonHoKang/Diffusion_Policy_with_Visual_Force_CrossAttn" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M17.25 6.75L22.5 12l-5.25 5.25m-10.5 0L1.5 12l5.25-5.25m7.5-3l-4.5 16.5"/></svg>
  Code
</a>


  
  
    
  
<a class="hb-attachment-link hb-attachment-small" href="https://huggingface.co/datasets/Jeon-hk/Battery_Prying_Dataset/blob/main/README.md" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M20.25 6.375c0 2.278-3.694 4.125-8.25 4.125S3.75 8.653 3.75 6.375m16.5 0c0-2.278-3.694-4.125-8.25-4.125S3.75 4.097 3.75 6.375m16.5 0v11.25c0 2.278-3.694 4.125-8.25 4.125s-8.25-1.847-8.25-4.125V6.375m16.5 0v3.75m-16.5-3.75v3.75m16.5 0v3.75C20.25 16.153 16.556 18 12 18s-8.25-1.847-8.25-4.125v-3.75m16.5 0c0 2.278-3.694 4.125-8.25 4.125s-8.25-1.847-8.25-4.125"/></svg>
  Dataset
</a>



<a class="hb-attachment-link hb-attachment-small" href="https://rros-lab.github.io/diffusion-with-force.github.io/" target="_blank" rel="noopener">
  Project
</a>




  


  
  
    
  
<a class="hb-attachment-link hb-attachment-small" href="https://www.youtube.com/watch?v=SyKixlaEUZY" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5" d="m15.75 10.5l4.72-4.72a.75.75 0 0 1 1.28.53v11.38a.75.75 0 0 1-1.28.53l-4.72-4.72M4.5 18.75h9a2.25 2.25 0 0 0 2.25-2.25v-9a2.25 2.25 0 0 0-2.25-2.25h-9A2.25 2.25 0 0 0 2.25 7.5v9a2.25 2.25 0 0 0 2.25 2.25Z"/></svg>
  Video
</a>





  </div>
  

  
  
</div>

  </div>
    
    
      <div class="publication-item"
          data-title="force-conditioned diffusion policies for compliant sheet separation tasks in bimanual robotic cells"
          data-summary="disassembly is a critical challenge in maintenance and service tasks, particularly in high-precision operations such as electric vehicle (ev) battery recycling.tasks like prying-open sealed battery covers require precise manipulation and controlled force application. in our approach we collect human demonstrations using a motion capture system, enabling the robot to learn from human-expert disassembly strategies.these demonstrations train a bimanual robotic system in which one arm exerts force with a specialized tool while the other manipulates and removes sealed components.our method builds on a diffusion-based policy and integrates real-time force sensing to adapt its actions as contact conditions change. we decompose the demonstrations into distinct sub-tasks and apply data augmentation, thereby reducing the number of demonstrations needed and mitigating potential task failures. our results show that the proposed method, even with a small dataset, achieves a high task success rate and efficiency compared to a standard diffusion technique. we demonstrate in a real-world application that the bimanual system effectively executes chiseling and peeling actions to separate bonded sheet from a substrate."
          data-author="rishabh shukla, samrudh mude, raj talan, neel dhanaraj, admin, satyandra k. gupta"
          data-year="2025">
    

    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Rishabh Shukla</span>, <span >
      Samrudh Mude</span>, <span >
      Raj Talan</span>, <span >
      Neel Dhanaraj</span>, <span class="font-bold">
      Jeon Ho Kang</span>, <span >
      Satyandra K. Gupta</span>
  </span>
  (2025).
  <a href="/publication/icra-2025/" class="underline">Force-Conditioned Diffusion Policies for Compliant Sheet Separation Tasks in Bimanual Robotic Cells</a>.
  ICRA 2025.
  

  
  <div class="flex flex-wrap space-x-3">
    








  





<a class="hb-attachment-link hb-attachment-small" href="/publication/icra-2025/cite.bib" target="_blank" data-filename="/publication/icra-2025/cite.bib">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"/></svg>
  Cite
</a>








  


  
  
    
  
<a class="hb-attachment-link hb-attachment-small" href="https://www.youtube.com/watch?v=rRBom3rPj3I" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5" d="m15.75 10.5l4.72-4.72a.75.75 0 0 1 1.28.53v11.38a.75.75 0 0 1-1.28.53l-4.72-4.72M4.5 18.75h9a2.25 2.25 0 0 0 2.25-2.25v-9a2.25 2.25 0 0 0-2.25-2.25h-9A2.25 2.25 0 0 0 2.25 7.5v9a2.25 2.25 0 0 0 2.25 2.25Z"/></svg>
  Video
</a>





  </div>
  

  
  
</div>

  </div>
    
    
      <div class="publication-item"
          data-title="generating and applying contingency handling procedures in human-robot teams in manufacturing applications"
          data-summary="in manufacturing, minimizing operational delays is crucial for efficiency and resilience. this paper introduces a novel approach to recover from contingencies by using large language models. the core of our approach leverages llms to enable quick and autonomous response to unforeseen contingencies. the results demonstrate advancements in\:(1) successful recovery from potential assembly operation setbacks, through tailored procedures; (2) the creation of adaptive, reactive strategies to overcome contingency, utilizing generative models; and (3) a significant reduction in human effort and makespan."
          data-author="admin, neel dhanaraj, siddhant wadaskar, satyandra k. gupta"
          data-year="2024">
    

    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="font-bold">
      Jeon Ho Kang</span>, <span >
      Neel Dhanaraj</span>, <span >
      Siddhant Wadaskar</span>, <span >
      Satyandra K. Gupta</span>
  </span>
  (2024).
  <a href="/publication/icra-2024/" class="underline">Generating and Applying Contingency Handling Procedures in Human-Robot Teams in Manufacturing Applications</a>.
  IEEE International Conference on Robotics and Automation (ICRA), 2024.
  

  
  <div class="flex flex-wrap space-x-3">
    








  



<a class="hb-attachment-link hb-attachment-small" href="/publication/icra-2024/ICRA%202024.pdf" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9"/></svg>
  PDF
</a>



<a class="hb-attachment-link hb-attachment-small" href="/publication/icra-2024/cite.bib" target="_blank" data-filename="/publication/icra-2024/cite.bib">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"/></svg>
  Cite
</a>








  


  
  
    
  
<a class="hb-attachment-link hb-attachment-small" href="https://www.youtube.com/watch?v=7eB1goPtUq4" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5" d="m15.75 10.5l4.72-4.72a.75.75 0 0 1 1.28.53v11.38a.75.75 0 0 1-1.28.53l-4.72-4.72M4.5 18.75h9a2.25 2.25 0 0 0 2.25-2.25v-9a2.25 2.25 0 0 0-2.25-2.25h-9A2.25 2.25 0 0 0 2.25 7.5v9a2.25 2.25 0 0 0 2.25 2.25Z"/></svg>
  Video
</a>





  </div>
  

  
  
</div>

  </div>
    
    
      <div class="publication-item"
          data-title="a task allocation and scheduling framework to facilitate efficient human-robot collaboration in high-mix assembly  applications"
          data-summary="automating assembly operations effectively increase efficiency while decreasing the need for humans to perform ergonomically challenging tasks. however, full automation of these tasks is still a work in progress. leveraging the complementary strengths of humans and robots offers a solution. humans can handle tasks requiring dexterity by working in a team, while robots undertake routine, supportive roles. however, contingency situations created by task execution failures occur more frequently in high-mix applications because of the high variability in the types of tasks. work cells that enable collaboration between humans and robots are not likely to be economically viable unless contingency situations are detected and efficiently managed. in such situations, additional contingency tasks must be executed to recover from the contingency, and productively utilizing human agents can help the work cell quickly recover from contingencies. this paper presents a framework for automatically assigning and scheduling tasks to humans and robots to complete multiple assemblies while managing the computational complexity of generating optimal task plans. additionally, we present methods to generate recovery task plans that effectively resolve those contingencies automatically. in our case study, we present an approach to graphically determine the number of tasks and products to consider with limited computing time."
          data-author="admin, neel dhanaraj, omey manyar, siddhant wadaskar, satyandra k. gupta"
          data-year="2024">
    

    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="font-bold">
      Jeon Ho Kang</span>, <span >
      Neel Dhanaraj</span>, <span >
      Omey Manyar</span>, <span >
      Siddhant Wadaskar</span>, <span >
      Satyandra K. Gupta</span>
  </span>
  (2024).
  <a href="/publication/msec-2024/" class="underline">A Task Allocation and Scheduling Framework to Facilitate Efficient Human-Robot Collaboration in High-Mix Assembly  Applications</a>.
  <strong>[Best Paper Award]</strong> Proceedings of ASME’s Manufacturing Science and Engineering Conference MSEC 2024 June 17-June 21, 2024, Knoxville TN, USA.
  

  
  <div class="flex flex-wrap space-x-3">
    








  



<a class="hb-attachment-link hb-attachment-small" href="/publication/msec-2024/MSEC%202024.pdf" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9"/></svg>
  PDF
</a>



<a class="hb-attachment-link hb-attachment-small" href="/publication/msec-2024/cite.bib" target="_blank" data-filename="/publication/msec-2024/cite.bib">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"/></svg>
  Cite
</a>








  






  </div>
  

  
  
</div>

  </div>
    
    
      <div class="publication-item"
          data-title="a learning framework for enabling robots to autonomously dispense granular material on-demand "
          data-summary="this paper presents a learning framework for enabling robots to autonomously dispense granular materials on demand. this framework enables robots to scoop and transfer the requested material amount with milligram scale accuracy. our approach is capable of handling challenging cases where the amount left in the source container is significantly less than the container volume. in such cases, robots must build piles before scooping the material to capture enough material within the scooper. we use gaussian process regression (gpr) to predict granular material behavior during scooping and pouring tasks. gpr is effective in learning the behavior of granular material with task parameters, such as robot joint angles, joint accelerations, and end-effector geometry. during task execution, we use gpr to solve the inverse problem and determine the task parameters based on the desired scooping and pouring amounts. the system performance is evaluated by showing gpr’s ability to predict scooped and poured amounts with reasonable uncertainty. we benchmark our method against the traditional approach of fine-tuning the amount via closed-loop control from the scale sensor feedback. our method shows 55.2% improvement in time taken to dispense the granular material over the benchmark approach. the proposed framework shows promising results in terms of reducing dispensing times."
          data-author="admin, rishabh shkla, moksh mehta, satyandra k. gupta"
          data-year="2024">
    

    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="font-bold">
      Jeon Ho Kang</span>, <span >
      Rishabh Shkla</span>, <span >
      Moksh Mehta</span>, <span >
      Satyandra K. Gupta</span>
  </span>
  (2024).
  <a href="/publication/idetc-2024/" class="underline">A Learning Framework for Enabling Robots to Autonomously Dispense Granular Material On-Demand </a>.
  DETC 2024 August 25–28, 2024 Washington, DC, USA.
  

  
  <div class="flex flex-wrap space-x-3">
    








  



<a class="hb-attachment-link hb-attachment-small" href="/publication/idetc-2024/iDETC%202024.pdf" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9"/></svg>
  PDF
</a>



<a class="hb-attachment-link hb-attachment-small" href="/publication/idetc-2024/cite.bib" target="_blank" data-filename="/publication/idetc-2024/cite.bib">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"/></svg>
  Cite
</a>





  
    
  




  






  </div>
  

  
  
</div>

  </div>
    
    
      <div class="publication-item"
          data-title="multi-robot task allocation under uncertainty via hindsight optimization"
          data-summary="multi-robot systems are becoming increasingly prevalent in various real-world applications, such as manufacturing and warehouse logistics. these systems face complex challenges in 1) task allocation due to factors like time-extended tasks, and agent specialization, and 2) uncertainties in task execution. potential task failures can add further contingency tasks to recover from the failure, thereby causing delays. this paper addresses the problem of multi-robot task allocation under uncertainty by proposing a hierarchical approach that decouples the problem into two levels. we use a low-level optimization formulation to find the optimal solution for a deterministic multi-robot task allocation problem with known task outcomes. the higher-level search intelligently generates more likely combinations of failures and calls the inner-level search repeatedly to find the optimal task allocation sequence, given the known outcomes. we validate our results in simulation for a manufacturing domain and demonstrate that our method can reduce the effect of potential delays from contingencies. we show that our algorithm is computationally efficient while improving average makespan compared to other baselines."
          data-author="neel dhanaraj, admin, anirban mukherjee, heramb nemlekar, stefanos nikolaidis, satyandra k. gupta"
          data-year="2024">
    

    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Neel Dhanaraj</span>, <span class="font-bold">
      Jeon Ho Kang</span>, <span >
      Anirban Mukherjee</span>, <span >
      Heramb Nemlekar</span>, <span >
      Stefanos Nikolaidis</span>, <span >
      Satyandra K. Gupta</span>
  </span>
  (2024).
  <a href="/publication/icra-2024-n/" class="underline">Multi-robot task allocation under uncertainty via hindsight optimization</a>.
  2024 IEEE International Conference on Robotics and Automation (ICRA).
  

  
  <div class="flex flex-wrap space-x-3">
    








  
    
  



<a class="hb-attachment-link hb-attachment-small" href="https://ieeexplore.ieee.org/abstract/document/10611370" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9"/></svg>
  PDF
</a>



<a class="hb-attachment-link hb-attachment-small" href="/publication/icra-2024-n/cite.bib" target="_blank" data-filename="/publication/icra-2024-n/cite.bib">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"/></svg>
  Cite
</a>








  






  </div>
  

  
  
</div>

  </div>
    
    
      <div class="publication-item"
          data-title="preference elicitation and incorporation for human-robot task scheduling"
          data-summary="in this work, we address the challenge of incorporating human preferences into the task-scheduling process for human-robot teams. humans have various individual preferences that can be influenced by context and situational information. incorporating these preferences can lead to improved team performance. our main contribution is a framework that helps elicit and incorporate preferences during task scheduling. we achieve this by proposing 1) a constraint programming method to generate a range of plans, 2) an intelligent approach for selecting and presenting task schedules based on task features, and 3) a preference incorporation method that uses large language models to convert preferences into soft constraints. our results demonstrate that we can efficiently generate diverse plans for preference elicitation and incorporate them into the task-scheduling process. we evaluate our framework using an assembly-inspired case study and show how it can effectively incorporate complex and realistic preferences. our implementation can be found at github.com/rros-lab/human-robot-preference-planning."
          data-author="neel dhanaraj, minseok jeon, admin, stefanos nikolaidis, satyandra k. gupta"
          data-year="2024">
    

    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      Neel Dhanaraj</span>, <span >
      Minseok Jeon</span>, <span class="font-bold">
      Jeon Ho Kang</span>, <span >
      Stefanos Nikolaidis</span>, <span >
      Satyandra K. Gupta</span>
  </span>
  (2024).
  <a href="/publication/case-2024/" class="underline">Preference Elicitation and Incorporation for Human-Robot Task Scheduling</a>.
  2024 IEEE 20th International Conference on Automation Science and Engineering (CASE).
  

  
  <div class="flex flex-wrap space-x-3">
    








  





<a class="hb-attachment-link hb-attachment-small" href="/publication/case-2024/cite.bib" target="_blank" data-filename="/publication/case-2024/cite.bib">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"/></svg>
  Cite
</a>








  






  </div>
  

  
  
</div>

  </div>
    
    
      <div class="publication-item"
          data-title="safe robot to human tool handover to support effective collaboration"
          data-summary="robot-to-human mechanical tool handover is a common task in a human-robot collaborative assembly where humans are performing complex, high-value tasks and robots are performing supporting tasks. this paper discusses an approach to ensure the safe handover of mechanical tools to humans. we introduce a framework to enable smart robotic assistants to safely and efficiently perform robot-to-human tool handovers. our system utilizes a specialized gripper design capable of firmly grasping objects with irregular geometries. we utilize a tool end detection method so that the robot grasps the tool end and ensures that the human can safely grab the handle during handover. additionally, the system is able to detect if the tool moves during the grasping process and either restart the pickup or account for the new orientation during hand-off planning. lastly, the hand-off planning ensures the robot releases the tool at the appropriate time when the human has safely grabbed the handle. our experimental results indicate that our system can safely and effectively hand off many different types of tools. we have tested the system’s ability to handle contingencies that may occur during the handover process successfully."
          data-author="admin, paolo limcaoco, neel dhanaraj, satyandra k. gupta"
          data-year="2023">
    

    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span class="font-bold">
      Jeon Ho Kang</span>, <span >
      Paolo Limcaoco</span>, <span >
      Neel Dhanaraj</span>, <span >
      Satyandra K. Gupta</span>
  </span>
  (2023).
  <a href="/publication/idetc-2023/" class="underline">Safe Robot to Human Tool Handover to Support Effective Collaboration</a>.
  In.
  

  
  <div class="flex flex-wrap space-x-3">
    








  



<a class="hb-attachment-link hb-attachment-small" href="/publication/idetc-2023/iDETC%202023.pdf" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9"/></svg>
  PDF
</a>



<a class="hb-attachment-link hb-attachment-small" href="/publication/idetc-2023/cite.bib" target="_blank" data-filename="/publication/idetc-2023/cite.bib">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 0 1-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 0 1 1.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 0 0-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 0 1-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 0 0-3.375-3.375h-1.5a1.125 1.125 0 0 1-1.125-1.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H9.75"/></svg>
  Cite
</a>





  
    
  




  


  
  
    
  
<a class="hb-attachment-link hb-attachment-small" href="https://www.youtube.com/watch?v=TKyEbTcFW5M" target="_blank" rel="noopener">
  <svg style="height: 1em" class='inline-block' xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5" d="m15.75 10.5l4.72-4.72a.75.75 0 0 1 1.28.53v11.38a.75.75 0 0 1-1.28.53l-4.72-4.72M4.5 18.75h9a2.25 2.25 0 0 0 2.25-2.25v-9a2.25 2.25 0 0 0-2.25-2.25h-9A2.25 2.25 0 0 0 2.25 7.5v9a2.25 2.25 0 0 0 2.25 2.25Z"/></svg>
  Video
</a>





  </div>
  

  
  
</div>

  </div>
    

    </div>
</div>


  </div>

  
  
  
</body>

<script>
  document.addEventListener('DOMContentLoaded', () => {
    const searchInput = document.getElementById('pubSearch');
    const authorInput = document.getElementById('pubAuthor');
    const yearInput = document.getElementById('pubYear');

    if (searchInput && authorInput && yearInput) {
      const items = document.querySelectorAll('.publication-item');

      function filterItems() {
        const query = searchInput.value.trim().toLowerCase();
        const author = authorInput.value.trim().toLowerCase();
        const year = yearInput.value.trim();

        items.forEach(item => {
          const title = item.dataset.title || "";
          const summary = item.dataset.summary || "";
          const authors = item.dataset.author || "";
          const pubYear = item.dataset.year || "";

          const matchQuery = title.includes(query) || summary.includes(query);
          const matchAuthor = authors.includes(author);
          const matchYear = pubYear.includes(year);

          const shouldShow = matchQuery && matchAuthor && matchYear;
          item.style.display = shouldShow ? "" : "none";
        });
      }

      searchInput.addEventListener("input", filterItems);
      authorInput.addEventListener("input", filterItems);
      yearInput.addEventListener("change", filterItems);
    }
  });
</script>

  

  
</section>

  

  
  
    








  

  

  

  

  


















  



  
  











  
    
    
    
  





























<section id="projects" class="relative hbb-section blox-collection  dark" style="padding: 6rem 0 6rem 0;" >
 <div class="home-section-bg " style="background-color: black;background-image: linear-gradient(30deg, #282B2C, #2c3e50);">
   
 </div>
  

  

    











  









  
  
  
  
  






















  









  <div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center px-6 md:px-0">

    <div class="mb-6 text-3xl font-bold text-gray-900 dark:text-white">
      Projects
    </div>

    
  </div>
  

  <div class="flex flex-col items-center px-6">

    
    


<div class="container px-8 mx-auto xl:px-5 py-5 lg:py-8 max-w-screen-lg ">
  <div class="grid gap-10 md:grid-cols-3 lg:gap-10">


    
    
      <div>
  

    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/project/residual-neural-networks/" >

      <img alt="Residual Neural Networks"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/project/residual-neural-networks/featured_hu_13854647b3ee623d.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/project/residual-neural-networks/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Residual Neural Networks
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/project/residual-neural-networks/" >
        GRU for name classification More insights and description to come!! :)
Code implementation can be found here - Github</a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2025-04-01">Apr 1, 2025</time>
        </div>
      </div>

    </div>
  </div>
</div>

  </div>
    
    
      <div>
  

    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/project/convolutional-neural-networks/" >

      <img alt="Convolutional Neural Networks"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/project/convolutional-neural-networks/featured_hu_366fa981d81fcecb.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/project/convolutional-neural-networks/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Convolutional Neural Networks
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/project/convolutional-neural-networks/" >
        U-net archiecture for image segmentation More insights and description to come!! :)
Code implementation can be found here - Github</a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2024-12-11">Dec 11, 2024</time>
        </div>
      </div>

    </div>
  </div>
</div>

  </div>
    
    
      <div>
  

    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/project/granular/" >

      <img alt="Robotic Granular Material Scooping Industry Project"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/project/granular/featured_hu_141e0de25e51f7f5.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/project/granular/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Robotic Granular Material Scooping Industry Project
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/project/granular/" >
        Granular Material Scooping with Sub-milliigram accuracy Check out our paper published to iDETC: ASME Paper Link
Open Access: Research Gate</a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2023-12-11">Dec 11, 2023</time>
        </div>
      </div>

    </div>
  </div>
</div>

  </div>
    
    
      <div>
  

    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/project/deep-neural-networks/" >

      <img alt="Deep Neural Networks"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/project/deep-neural-networks/featured_hu_8ed824c7a98c3f1e.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/project/deep-neural-networks/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Deep Neural Networks
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/project/deep-neural-networks/" >
        Training neural network from scratch</a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2023-11-01">Nov 1, 2023</time>
        </div>
      </div>

    </div>
  </div>
</div>

  </div>
    
    
      <div>
  

    









<div class="group cursor-pointer">

  
  
  
    
  
  
  <div class="overflow-hidden rounded-md bg-gray-100 transition-all hover:scale-105 dark:bg-gray-800">

    <a
      class="relative block aspect-video"
      href="/project/3d-vision/" >

      <img alt="Pointcloud-based Pick and Place"
           class="object-fill transition-all"
           data-nimg="fill"
           decoding="async"
           fetchpriority="high" height="540" loading="lazy" src="/project/3d-vision/featured_hu_4bce8c86eeed2d57.webp"
           style="position: absolute; height: 100%; width: 100%; inset: 0px; color: transparent;"
           width="960"></a>
  </div>
  
  <div class="">
    <div class="">
      <div class="flex gap-3">
        
      </div>
      
      <h2 class="text-lg font-semibold leading-snug tracking-tight mt-2 dark:text-white"><a
        href="/project/3d-vision/" ><span
        class="bg-gradient-to-r from-primary-200 to-primary-100 bg-[length:0px_10px] bg-left-bottom bg-no-repeat transition-[background-size] duration-500 hover:bg-[length:100%_3px] group-hover:bg-[length:100%_10px] dark:from-primary-800 dark:to-primary-900">Pointcloud-based Pick and Place
          </span></a>
      </h2>
      
      <div class="grow"><p class="mt-2 line-clamp-3 text-sm text-gray-500 dark:text-gray-400"><a
        href="/project/3d-vision/" >
        Point cloud-based pose-estimation and segmentation for Pick and Place task </a></p>
      </div>
      <div class="flex-none">
        <div class="mt-3 flex items-center space-x-3 text-gray-500 dark:text-gray-400 cursor-default">
          
          
          <time class="truncate text-sm" datetime="2022-06-11">Jun 11, 2022</time>
        </div>
      </div>

    </div>
  </div>
</div>

  </div>
    

      </div>
</div>


  </div>

  
  
  
</body>

<script>
  document.addEventListener('DOMContentLoaded', () => {
    const searchInput = document.getElementById('pubSearch');
    const authorInput = document.getElementById('pubAuthor');
    const yearInput = document.getElementById('pubYear');

    if (searchInput && authorInput && yearInput) {
      const items = document.querySelectorAll('.publication-item');

      function filterItems() {
        const query = searchInput.value.trim().toLowerCase();
        const author = authorInput.value.trim().toLowerCase();
        const year = yearInput.value.trim();

        items.forEach(item => {
          const title = item.dataset.title || "";
          const summary = item.dataset.summary || "";
          const authors = item.dataset.author || "";
          const pubYear = item.dataset.year || "";

          const matchQuery = title.includes(query) || summary.includes(query);
          const matchAuthor = authors.includes(author);
          const matchYear = pubYear.includes(year);

          const shouldShow = matchQuery && matchAuthor && matchYear;
          item.style.display = shouldShow ? "" : "none";
        });
      }

      searchInput.addEventListener("input", filterItems);
      authorInput.addEventListener("input", filterItems);
      yearInput.addEventListener("change", filterItems);
    }
  });
</script>

  

  
</section>

  

  
  
    








  

  

  

  

  


















  



  
  











  
    
    
    
  





























<section id="skills" class="relative hbb-section blox-resume-skills  custom-position dark" style="padding: 6rem 0 6rem 0;" >
 <div class="home-section-bg " style="background-color: black;background-image: linear-gradient(30deg, #282B2C, #2c3e50);">
   
 </div>
  

  

    














<div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center">

  <div class="mb-6 text-4xl font-bold text-gray-900 dark:text-white">
    Skills &amp; courses
  </div>

  
</div>

<div class="flex flex-col lg:flex-row items-center max-w-prose mx-auto gap-3 px-6 md:px-0">

  
  
  
  <div class="w-full lg:w-1/2 px-8">
    <div class="mb-5 text-2xl font-bold text-gray-900 dark:text-white">
      Technical Skills
      
    </div>
    
    
    
    
    
    

    <div class="skills-content">

      

      <span class="skills-name text-gray-700 dark:text-gray-300 text-5xl font-semibold">
        Programming
        <p class="skills-description">C/C++, Python, Linux</p>
      </span>
      
    </div>
    
    
    
    
    
    

    <div class="skills-content">

      

      <span class="skills-name text-gray-700 dark:text-gray-300 text-5xl font-semibold">
        Libraries
        <p class="skills-description">Pytorch, TensorFlow, MoveIt, ROS/ROS2,</br> OpenCV, Open3D</p>
      </span>
      
    </div>
    
    
    
    
    
    

    <div class="skills-content">

      

      <span class="skills-name text-gray-700 dark:text-gray-300 text-5xl font-semibold">
        Simulation Software
        <p class="skills-description">Isaac Sim, Mujoco, Webots, Gazebo</p>
      </span>
      
    </div>
    
    
    
    
    
    

    <div class="skills-content">

      

      <span class="skills-name text-gray-700 dark:text-gray-300 text-5xl font-semibold">
        Robots
        <p class="skills-description">Kuka, ABB, Yasakawa, UR</p>
      </span>
      
    </div>
    
    
    
    
    
    

    <div class="skills-content">

      

      <span class="skills-name text-gray-700 dark:text-gray-300 text-5xl font-semibold">
        Developer Tools
        <p class="skills-description">Git, CUDA, Docker</p>
      </span>
      
    </div>
    
  </div>
    
  
  
  <div class="w-full lg:w-1/2 px-8">
    <div class="mb-5 text-2xl font-bold text-gray-900 dark:text-white">
      Courses
      
    </div>
    
    
    
    
    
    

    <div class="skills-content">

      

      <span class="skills-name text-gray-700 dark:text-gray-300 text-5xl font-semibold">
        CS 182 - Designing, Visualizing and Understanding Deep Neural Networks
        <p class="skills-description">Great course taught by Professor Sergey Levine: <a href="https://cs182sp21.github.io" target="_blank" rel="noopener">Syllabus</a>. Spans from basic affine transform, regularization techniques. Then goes in to different architectures like CNN, RNN and Transformers. Touches on different applications like natural language processing, computer vision and imitation learning for self-driving cars and robotics.</p>
      </span>
      
    </div>
    
    
    
    
    
    

    <div class="skills-content">

      

      <span class="skills-name text-gray-700 dark:text-gray-300 text-5xl font-semibold">
        NN-zero-to-hero
        <p class="skills-description">Considered one of the best tutorials to neural networks taught by <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ" target="_blank" rel="noopener">Andrej Karpathy</a>.</p>
      </span>
      
    </div>
    
    
    
    
    
    

    <div class="skills-content">

      

      <span class="skills-name text-gray-700 dark:text-gray-300 text-5xl font-semibold">
        AME 540 - Probability and Statistics for Engineering Science
        <p class="skills-description">Course taught at USC by Professor <a href="https://viterbi.usc.edu/directory/faculty/Oberai/Assad" target="_blank" rel="noopener">Assad Obeai</a>. Touches on introductory theories behind probability like random variables and vectors, Conditional distributions and Bayes theorem, and stocahstic processes and its applications</p>
      </span>
      
    </div>
    
  </div>
    
</div>

  

  
</section>

  

  
  
    








  

  

  

  

  


















  



  
  











  
    
    
    
  





























<section id="awards" class="relative hbb-section blox-resume-awards  dark" style="padding: 6rem 0 6rem 0;" >
 <div class="home-section-bg " style="background-color: black;background-image: linear-gradient(30deg, #282B2C, #2c3e50);">
   
 </div>
  

  

    













<div class="flex flex-col items-center max-w-prose mx-auto gap-3 justify-center">

  <div class="mb-6 text-3xl font-bold text-gray-900 dark:text-white">
    Accomplishments
  </div>

  

<div class="w-full flex flex-col gap-6">


<div class="w-full p-6 bg-gray-600 rounded-2xl shadow-xl text-gray-900 dark:bg-gray-200 dark:text-gray-900">
  
  
    <h5 class="mb-2 text-2xl font-semibold tracking-tight text-gray-900 dark:text-white">Best Conference Paper Award (2nd Place)</h5>
  

  <div class="block mb-3 text-sm font-normal leading-none text-gray-500 dark:text-gray-300">
    The American Society of Mechanical Engineers - ASME &#8729;
    
      November 2023
    
  </div>

  

  
</div>
  
<div class="w-full p-6 bg-gray-600 rounded-2xl shadow-xl text-gray-900 dark:bg-gray-200 dark:text-gray-900">
  
  
    <h5 class="mb-2 text-2xl font-semibold tracking-tight text-gray-900 dark:text-white">NSF Travel Award 2024</h5>
  

  <div class="block mb-3 text-sm font-normal leading-none text-gray-500 dark:text-gray-300">
    National Science Foundation (NSF) &#8729;
    
      September 2023
    
  </div>

  

  
</div>
  
<div class="w-full p-6 bg-gray-600 rounded-2xl shadow-xl text-gray-900 dark:bg-gray-200 dark:text-gray-900">
  
  
    <h5 class="mb-2 text-2xl font-semibold tracking-tight text-gray-900 dark:text-white">Research Featured in <a href="https://spectrum.ieee.org/video-friday-lunar-base" target="_blank" rel="noopener">IEEE Video Friday</a></h5>
  

  <div class="block mb-3 text-sm font-normal leading-none text-gray-500 dark:text-gray-300">
    IEEE Spectrum &#8729;
    
      March 2023
    
  </div>

  

  
</div>
  
  
</div>
</div>

  

  
</section>

  


    </div>
    <div class="page-footer">
      <footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200">

  












  
  
  
  
  














  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by text-center">
    © 2025 Jeon Kang.
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by text-center">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>

</footer>

    </div>

    
    








  




  </body>
</html>
